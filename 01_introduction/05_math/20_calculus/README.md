# 微积分

### 梯度计算

#### autograd()

在 PyTorch 中，一般用 autograd() 函数计算梯度，它使得神经网络的反向传播计算更加方便和高效。

1. Tensor（张量）：在 PyTorch 中，autograd 的核心数据结构是 Tensor，它是包含了数值和相关操作历史的多维数组。通过对 Tensor 执行操作，可以构建计算图。
2. 计算图：计算图是由一系列 Tensor 运算组成的DAG。每个 Tensor 表示图中的一个节点，而操作则表示图中的边。计算图记录了操作的依赖关系，使得 PyTorch 可以追踪每个操作的输入和输出。
3. 反向传播：在计算图中，当执行前向传播计算时，autograd 自动构建了图中的每个操作的导数计算函数。当计算完成后，可以通过调用 `backward()` 方法来自动计算所有业务节点的梯度。这个过程使用链式法则（chain rule）来计算导数，并将梯度传播回计算图的叶子节点。
4. 叶子节点：在计算图中，被设置为 `requires_grad=True` 的 Tensor 被认为是叶子节点，即这些节点是计算图的起点。叶子节点的梯度需要通过调用 `backward()` 方法来计算。

计算梯度示例

```python
import torch

# 创建一个张量，设置 requires_grad=True
x = torch.tensor([2.0], requires_grad=True)

# 定义一个函数 y = x^2
y = x**2

# 计算梯度，并会把计算后的梯度存储到x.grad中
y.backward()

# 打印 x 的梯度
print(x.grad)
```

在上例中，当调用 `y.backward()` 时，PyTorch 的 autograd 会根据计算图中的操作历史自动计算梯度：

1. 创建张量和设置 `requires_grad=True`：在示例中，我们创建了一个张量 `x` 并设置 `requires_grad=True`，这表示我们希望计算 `x` 的梯度。
2. 前向传播计算：我们定义了一个函数 `y = x**2`，即 y 是 x 的平方。在计算 `y` 时，autograd 会跟踪并记录计算图中的每个操作。
3. 计算图构建：autograd 通过构建计算图来追踪操作的依赖关系。计算图是一个DAG，其中每个节点是一个 Tensor，而边表示操作。
4. 反向传播计算梯度：当调用 `y.backward()` 时，autograd 会自动计算梯度。具体而言，它会从计算图的末尾节点 `y` 开始，通过链式法则依次计算每个操作的导数，并将梯度传播回计算图的叶子节点 `x`。在示例中，计算的梯度是 `dy/dx = 2x`。
5. 梯度存储：计算得到的梯度将存储在叶子节点的 `grad` 属性中。在示例中，我们可以通过访问 `x.grad` 来获取 `x` 的梯度值。

#### 具体梯度计算

- 符号计算（symbolic computation）：首选方案，通过计算函数的导数来计算梯度。
- 数值差分（numerical difference）：在 PyTorch 中，当遇到无法进行符号计算的函数时，autograd 会自动使用数值差分来计算梯度。数值差分方法通过在函数上取一小步的变化并计算相应的输出变化来近似计算导数。需要注意的是，数值差分方法是一种近似计算方法，相对于符号计算，它可能会引入一些近似误差。此外，数值差分方法在计算梯度时可能会比符号计算方法更加耗时。因此，在可能的情况下，PyTorch 会优先使用符号计算来计算梯度，以提高计算效率和精确性。

